{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = str('C:/Users/razkey/git/HybridMem-Scheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/request\n",
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/trafficGen\n",
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/page\n",
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/addressSpace\n",
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/scheduler\n",
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/pageSelector\n",
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/profile\n",
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/performanceModel\n"
     ]
    }
   ],
   "source": [
    "includes = ['request','trafficGen','page','addressSpace','scheduler','pageSelector','profile','performanceModel']\n",
    "files=[]\n",
    "for component in includes:\n",
    "    file = workspace+'/MemProfiling/'+component\n",
    "    print(file)\n",
    "    %run $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/LSTM/LSTM_input\n",
      "C:/Users/razkey/git/HybridMem-Scheduler/MemProfiling/LSTM/lstm\n"
     ]
    }
   ],
   "source": [
    "lstmIncludes = ['LSTM_input','lstm']\n",
    "files=[]\n",
    "for component in lstmIncludes:\n",
    "    file = workspace+'/MemProfiling/LSTM/'+component\n",
    "    print(file)\n",
    "    %run $file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available benchmarks\n",
    "benchmarks= [\"backprop_100k\",\"streamcluster\",\"lud_2048\",\"blackscholes\",\"bodytrack\",\"bplustree\",\"hotspot_1024\",\"kmeans_80k\"]\n",
    "\n",
    "#Choose trace file\n",
    "trace_file=workspace+'/traces/'+benchmarks[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read Trace, Convert Input raw txt to python objects with useful attributes\n",
    "'''\n",
    "\n",
    "prof = Prof(trace_file)\n",
    "prof.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run History Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scheduler] Initialization done for policy = history periods = 101 reqs per period = 5892 cap ratio = 0.125\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " GET Per Page Access Counts using a History Page Scheduler\n",
    " necessary step for Page Selector\n",
    "\n",
    " epochs=100 , input file is broken into 100 epochs\n",
    " policy=history , history scheduler is used\n",
    " cap_ratio=1/8 , 1/8 fit in DRAM , 7/8 in NVM \n",
    "'''\n",
    "\n",
    "epochs = 100\n",
    "num_reqs = len(prof.traffic.req_sequence)//epochs\n",
    "sim = PerformanceModel(prof,'TEST-HMS',policy='history',cap_ratio=1/8,num_reqs=len(prof.traffic.req_sequence)//epochs)\n",
    "sim.init()\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Page Selector\n",
    "We now run Page Selector (afte running the History Scheduler) to find which pages are heavily misplaced and then choose the ones we want for RNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GET Misplaced pages eligible for RNN training \n",
    "'''\n",
    "page_selector = PageSelector(prof, 'TEST-HMS', '0.25', num_reqs)\n",
    "pages_misplaced = page_selector.get_misplaced_pages_sim()\n",
    "pages_ordered = page_selector.get_ordered_pages(pages_misplaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1756\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "How many RNNs to deploy\n",
    "'''\n",
    "page_id_x = pages_ordered[:1] #Chose one as a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN training\n",
    "Now we train RNN for the selected pages. I only train one RNN for the most heavily misplaced page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make the RNN input\n",
    "# Step 1: take the page access count across periods\n",
    "cnts_x = prof.hmem.page_list[page_id_x[0]].oracle_counts_binned_ep\n",
    "input = LSTM_input(cnts_x)\n",
    "  \n",
    "# Step 2: Roll a window of history length over the periods\n",
    "history_length = 20 # periods\n",
    "input.timeseries_to_history_seq(history_length)\n",
    "  \n",
    "# Step 3: Split into training, validation and test samples througout the epochs\n",
    "input.split_data(0.15)\n",
    "  \n",
    "# Step 4: Bring input into format for RNN training\n",
    "num_classes = max(set(cnts_x)) + 1\n",
    "input.to_categor(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Init and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initiliaze model\n",
    "model = LSTM_model(input)\n",
    "\n",
    "#initialize weights for custom loss function\n",
    "weights=[]\n",
    "for i in range(num_classes):\n",
    "    if i==0:\n",
    "        weights.append(30)\n",
    "    else:\n",
    "        weights.append(1)\n",
    "model.weights=weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_11 (LSTM)              (None, 20, 256)           336896    \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 256)               525312    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 72)                18504     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 880,712\n",
      "Trainable params: 880,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\razkey\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "### Create model\n",
    "#Setup weights for loss function\n",
    "model.weights=weights\n",
    "\n",
    "###arg1(layers)=256 , learning rate(arg2)=0.001, history_length(arg3)=20, num_classes(arg4), weights(arg5) weights for every class\n",
    "model.create(layers=256,learning_rate=0.0001, dropout=0,history_length=history_length,num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1756\n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "\n",
    "#model is an object of type LSTM_model which contains a keras.sequential model\n",
    "print(page_id_x[0])\n",
    "model.model.save(workspace+'MemProfiling/models/'+'model'+str(page_id_x[0])+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(workspace+'/models/'+'model'+str(page_id_x[0])+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#How to use a trained model for predictions \n",
    "\n",
    "import random\n",
    "cnts_x = prof.hmem.page_list[page_id_x[0]].oracle_counts_binned_ep\n",
    "#num_classes=72\n",
    "#history_length=20\n",
    "\n",
    "\n",
    "accessCounts=[]\n",
    "for x in range(0,21):\n",
    "    accessCounts.append(random.randint(0,71))\n",
    "print(len(accessCounts))\n",
    "sampleinput = LSTM_input(accessCounts)\n",
    "\n",
    "sampleinput.timeseries_to_history_seq(20)\n",
    "sampleinput.split_data(1)\n",
    "sampleinput.to_categor(72)\n",
    "\n",
    "\n",
    "\n",
    "predictY = loaded_model.predict(sampleinput.trainX_categor)\n",
    "predictY1 = np.array([np.argmax(x) for x in predictY])\n",
    "#print(predictY)\n",
    "print(predictY1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load all the models \n",
    "# I only have one so I load only one it's for page 1756\n",
    "loadedModels=[]\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(workspace+'/models/'+'model'+str(page_id_x[0])+'.h5')\n",
    "#loadedModels.append(loaded_model)\n",
    "\n",
    "#Create a dictionary with (page_id:model)\n",
    "dic={}\n",
    "dic[1756]=loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scheduler] Initialization done for policy = hybrid periods = 101 reqs per period = 5892 cap ratio = 0.125\n"
     ]
    }
   ],
   "source": [
    "#Now assuming we have a dictionary with all the trained models + the page id\n",
    "\n",
    "# Relaod the trace\n",
    "trace_file=workspace+'/traces/backprop_100k'\n",
    "prof = Prof(trace_file)\n",
    "prof.init()\n",
    "epochs = 100\n",
    "num_reqs=len(prof.traffic.req_sequence)//epochs\n",
    "\n",
    "sim_hybrid = PerformanceModel(prof,'Hybrid-HMS','hybrid',1/8,num_reqs=num_reqs)\n",
    "sim_hybrid.init()\n",
    "sim_hybrid.init_hybrid(dic)\n",
    "sim_hybrid.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET INFO FOR EVERY PAGE\n",
    "page_list = sim_hybrid.profile.hmem.page_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'req_ids': [0, 2, 4, 9, 10, 24, 25, 252, 1053, 1274, 1275, 3044, 3045], 'pc_ids': [], 'address': '140065108709376', 'reuse_dist': array([   2,    2,    5,    1,   14,    1,  227,  801,  221,    1, 1769,\n",
      "          1]), 'misplacements': 0, 'misplacementsPeriods': [], 'writesPerPeriod': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'readsPerPeriod': array([13.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), 'oracle_counts_ep': array([13.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), 'oracle_counts_binned_ep': [12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'pred_counts_binned_ep': array([ 0., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), 'counts_ep': array([13.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), 'loc_ep': array([0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n"
     ]
    }
   ],
   "source": [
    "for x in page_list:\n",
    "    print(vars(x))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
